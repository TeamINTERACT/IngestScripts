{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:20:15.556097Z",
     "start_time": "2020-11-10T17:20:05.571017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:52:48] → Initial parameters registered. You are about to ingest Wave 2 Ethica data for the city of Saskatoon.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import subprocess as sub\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import matplotlib\n",
    "\n",
    "from tabulate import tabulate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Environment variables like INGEST_WAVE and INGEST_CITY should be set \n",
    "# in your .env file before running. See example.env for intended variables\n",
    "\n",
    "# expects to find connection credentials in local runtime environment\n",
    "db_host=os.environ['SQL_LOCAL_SERVER']\n",
    "db_host_port=int(os.environ['SQL_LOCAL_PORT'])\n",
    "db_user=os.environ['SQL_USER']\n",
    "city_num=int(os.environ['INGEST_CITY'])\n",
    "wave_num=int(os.environ['INGEST_WAVE'])\n",
    "db_name=os.environ['DB_NAME']\n",
    "db_schema=os.environ['DB_SCHEMA']\n",
    "\n",
    "city_data = {1:{'prefix':'vic', 'name':'Victoria'},\n",
    "             2:{'prefix':'van', 'name':'Vancouver'},\n",
    "             3:{'prefix':'ssk', 'name':'Saskatoon'},\n",
    "             4:{'prefix':'mtl', 'name':'Montreal'}}\n",
    "\n",
    "study_ids = {1:[730,],\n",
    "             2:[1058,],\n",
    "             3:[1318,],\n",
    "             4:[1082,1083],}\n",
    "\n",
    "# these should be derivable from above settings\n",
    "wavenumstr = f\"{wave_num:02}\"\n",
    "city_prefix = city_data[city_num]['prefix']\n",
    "city_name = city_data[city_num]['name']\n",
    "datadir = f\"/scratch/lazarou/test_pipeline/permanent_archive/{city_name}/Wave{int(wavenumstr)}/Ethica\"\n",
    "telemetrydir = f\"{datadir}/raw\"\n",
    "linkage_file = f\"{datadir}/../linkage.csv\"\n",
    "\n",
    "# Create a list of users who are to be specifically excluded from a particular study\n",
    "# It's being done this way because the linkage CSV files do not encode anything about study numbers, and\n",
    "# adding such a feature would break a lot of code, plus risk contaminating the CSV files through sloppy changes\n",
    "# So since these cases are rare, I'm putting them here, at the head of the code where they should be \n",
    "# obvious and apparent.\n",
    "suppressors = {} #none needed for Mtl W2\n",
    "\n",
    "\n",
    "def log(instr):\n",
    "    timestr = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \")\n",
    "    print(f\"{timestr}→ {instr}\")\n",
    "    \n",
    "log(f\"Initial parameters registered. You are about to ingest Wave {wave_num} Ethica data for the city of {city_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the block:** The output line above is telling you which city, wave, and telemetry type you are about to ingest. *Read that line carefully!* If any of it is wrong, you've probably forgotten to set your environment variables correctly. You can either: shut down Jupyter Lab, update your environment, and then re-launch Jupyter; or you can override settings using a %env statement before the environment values are checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "city_name": "Victoria"
    }
   },
   "source": [
    "# Ingest Log for Ethica Wave 2\n",
    "\n",
    "After much investigation and discussion, we have decided to ingest the data in its raw form, with no filters applied, and then create a few obvious subtables from that ingested data. The reasoning is that, due to the nature of the data issues, there appears to be no single cleaning filter we can apply that will be appropriate for all downstream analysis. So we'll ingest the raw table, mark it as toxic so nobody uses it by accident, and then create a few sane starting points that people can actually use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep\n",
    "Steps to complete before ingesting the telemetry\n",
    "\n",
    "- Ensure that any users to be re-ingested from linkage have been removed from ethica_assignments table\n",
    "- Verify the integrity of the linkage CSV file before ingesting it\n",
    "- Ingest the linkage CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Linkage CSV\n",
    "This step will assess the CSV file to ensure it's in a parsable format and then confirm that the users listed  match those who have contributed telemetry data. If any errors or mismatches are reported here, the linkage file records should be updated until this verification test passes cleanly.\n",
    "\n",
    "#### Possible problems and their solutions\n",
    "\n",
    "##### User in linkage file but has no telemetry data\n",
    "This can happen when a user was assigned an Ethica account, but either dropped out of the study or switched to SenseDoc before data collection began. If the study coordinator confirms that the user dropped out and produced no usable data, then mark the user with 'ignore' in the data_disposition field of the linkage file. This will exclude them from the list of ids who are expected to have telemetry data.\n",
    "\n",
    "##### User has telemetry data but is not in linkage file\n",
    "This usually happens when data got collected from a staff member who was testing the system. For these people, there *is* data, but we want to remove it from the ingest. So we don't just ignore these records, we have to actively cull the associated data. This is indicated by marking the data_disposition field with 'cull'.\n",
    "\n",
    "**Should the information be placed in the linkage file or the linkage table?** The file is used at ingest validation time, while the table is used at actual telemetry ingest time, so probably both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:20:15.608939Z",
     "start_time": "2020-11-10T17:20:15.573202Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:53:03] → Code has been loaded.\n"
     ]
    }
   ],
   "source": [
    "# Helper functions that will be used to verify the linkage CSV file\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "iids = set([])\n",
    "eids = set([])\n",
    "culled_eids = set([])\n",
    "participant_study = {} # a list of which study each participant is registered in\n",
    "\n",
    "# In one city, the linkage encode ethica id as 'Ethica ID'\n",
    "# In another, it is encoded as 'ethica_id'\n",
    "# We may want to make that column name an input parameter\n",
    "\n",
    "def validate_iid(idstr, logfailures=True):\n",
    "    \"\"\"\n",
    "    Given an Interact ID in string form, ensure it's well formed.\n",
    "    Return True if so, False otherwise.\n",
    "    \"\"\"\n",
    "    if not len(idstr.strip()) > 8:\n",
    "        if logfailures: log(f\"Interact ID '{idstr}' is not long enough.\")\n",
    "        return False\n",
    "    try:\n",
    "        iid = int(idstr)\n",
    "    except ValueError:\n",
    "        if logfailures: log(f\"Interact ID '{idstr}' is not an integer.\")\n",
    "        return False\n",
    "    global iids\n",
    "    iids.add(iid)\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_eid(idstr, logfailures=True):\n",
    "    \"\"\"\n",
    "    Given an Ethica ID in string form, ensure it's well formed and\n",
    "    unique. Return True if so, False otherwise.\n",
    "    \"\"\"\n",
    "    if not(idstr):\n",
    "        if logfailures: log(f\"Ethica ID is empty. Skipping.\")\n",
    "        return False\n",
    "    try:\n",
    "        eid = int(idstr)\n",
    "    except ValueError: # filter B\n",
    "        if logfailures: log(f\"Ethica ID '{idstr}' is not an integer. Skipping.\")\n",
    "        return False\n",
    "    global eids\n",
    "    if eid in eids: # filter A\n",
    "        if logfailures: log(f\"Ethica ID '{idstr}' already ingested.\")\n",
    "        return False\n",
    "    eids.add(str(eid)) \n",
    "    # we only converted to int to be sure the string was int-friendly\n",
    "    # all subsequent tests against id will be done as string compares\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_wear_dates(row):\n",
    "    \"\"\"\n",
    "    Given an Ethica record, ensure it has well-formed start and\n",
    "    end dates. Return True if so, False otherwise.\n",
    "                            DO NOT USE!\n",
    "    Wear dates are only valid for SenseDoc data. With SD, data\n",
    "    capture begins the moment the device is booted, so it captures\n",
    "    activity of the coordinator prior to delivery to the participant,\n",
    "    and then again after the device has been recovered. Wear dates\n",
    "    are used in that situation to filter out the coordinator data.\n",
    "    But Ethica data is different and has no coordinator \"pollution\"\n",
    "    that needs to be removed.\n",
    "\n",
    "    I'm leaving this code here though, with this comment, so that\n",
    "    nobody is tempted to add date filters back in at a later time.\n",
    "    \"\"\"\n",
    "    return True\n",
    "    try:\n",
    "        start_str = row['start_date']\n",
    "        end_str = row['end_date']\n",
    "        start_date = datetime.strptime(start_str,\"%Y-%m-%d\") \n",
    "        end_date = datetime.strptime(end_str,\"%Y-%m-%d\") \n",
    "    except KeyError:\n",
    "        log(f\"EID '{row['ethica_id']}' has missing date field(s)\")\n",
    "        return False\n",
    "    except ValueError:\n",
    "        log(f\"EID '{row['ethica_id']}' has missing/invalid date info {start_str} - {end_str}\")\n",
    "        return False\n",
    "    if start_date >= end_date:\n",
    "        log(f\"EID '{row['ethica_id']}' has invalid date window {start_str} - {end_str}\")\n",
    "        return False\n",
    "    log(f\"EID '{row['ethica_id']}' has good date window {start_str} - {end_str}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_possible_values_from_csv(filename, colnum):\n",
    "    \"\"\"\n",
    "    Given a CSV file and the 1-based index number of a column,\n",
    "    return a list of all unique values in that column\n",
    "    \"\"\"\n",
    "    values = set()\n",
    "    # To show a helpul progress bar, we have to know how many\n",
    "    # lines need to be processed, so we need to traverse the file\n",
    "    # twice. This might seem like overkill, but the data files are\n",
    "    # pretty big and can take hours to process. Showing signs of\n",
    "    # life on the console while the script is churning away silently\n",
    "    # in the background can be very reassuring.\n",
    "    if os.path.isfile(filename):\n",
    "        log(f\"Scanning file: {filename}\")\n",
    "    else:\n",
    "        log(f\"WARNING: Unable to find file '{filename}'\")\n",
    "        return values\n",
    "    #nlines = sum(1 for x in open(filename))\n",
    "    #with open(filename,'r',encoding='ISO-8859-1') as fcsv:\n",
    "    #    reader = csv.DictReader(fcsv,delimiter=',')\n",
    "    #    for row in reader:\n",
    "    #        values.add(row[colname])\n",
    "            \n",
    "    # TURNS OUT THE ABOVE IS HORRIBLY INEFFICIENT\n",
    "    # IT'S ORDERS OF MAGNITUDE FASTER TO RUN A SHELL SCRIPT LIKE THIS:\n",
    "    # cut -d, -f 1 Ethica/montreal_en_01/raw/gps.csv | sort -u\n",
    "    \n",
    "    # set up the piped commands\n",
    "    skipcmd = ['tail', '-n', '+2', filename] # skip the first line, which has column names\n",
    "    listuseridscmd = ['cut', '-d,', '-f', colnum] # user_id = given field of each row\n",
    "    distinctifycmd = ['sort', '-u'] # create a list of distinct ids\n",
    "    #log(f\"skipcmd: {skipcmd}\")\n",
    "    #log(f\"Listuseridscmd: {listuseridscmd}\")\n",
    "    #log(f\"Distinctifycmd: {distinctifycmd}\")\n",
    "    \n",
    "    # run the piped commands\n",
    "    p0 = subprocess.Popen(skipcmd, stdout=subprocess.PIPE)\n",
    "    p1 = subprocess.Popen(listuseridscmd, stdin=p0.stdout, stdout=subprocess.PIPE)\n",
    "    p2 = subprocess.Popen(distinctifycmd, stdin=p1.stdout, stdout=subprocess.PIPE) #, encoding='utf8')\n",
    "    #p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.\n",
    "    output,err = p2.communicate()\n",
    "    \n",
    "    # parse the output\n",
    "    values = []\n",
    "    for val in output.decode('utf-8').split('\\n'):\n",
    "        if val: # skip empty lines\n",
    "            if val.isnumeric():\n",
    "                values.append(val)\n",
    "            else:\n",
    "                log(f\"Unexpected non-numeric entry in column {colnum} = '{val}'\")\n",
    "\n",
    "    return values\n",
    "\n",
    "# Quick test of the above routine\n",
    "#log(\"Testing function: get_possible_values_from_csv()\")\n",
    "#foo = get_possible_values_from_csv(os.path.join(datadir, 'montreal_en_01/raw/gps.csv'), '1')\n",
    "#log(f\"Found {len(foo)} users in English data.\")\n",
    "#foo = get_possible_values_from_csv(os.path.join(datadir, 'montreal_fr_01/raw/gps.csv'), '1')\n",
    "#log(f\"Found {len(foo)} users in French data.\")\n",
    "\n",
    "def detect_missing_values(master_values, csvfilelist, colnum, colname, streamname):\n",
    "    \"\"\"\n",
    "    Compare the values in a particular column of a CSV against a\n",
    "    master list of expected values and report values in the CSV\n",
    "    that do not match the expectation.\n",
    "    \"\"\"\n",
    "    found_values = set([])\n",
    "    for csvfile in csvfilelist:\n",
    "        studynum = csvfilelist[csvfile]\n",
    "        log(f\"File {csvfile} is associated with study number {studynum}\")\n",
    "        users_in_file = get_possible_values_from_csv(csvfile, colnum)\n",
    "        for user in users_in_file:\n",
    "            if user in participant_study and participant_study[user] != studynum:\n",
    "                log(f\"WARNING: User {user} has data in conflicting studies: {studynum} and {participant_study[user]}\")\n",
    "            participant_study[user] = studynum         \n",
    "        found_values.update(users_in_file)\n",
    "        \n",
    "    # convert items to ints so they can be compared with master_values\n",
    "    found_values = set([int(x) for x in found_values])\n",
    "    \n",
    "    #log(f\"Found values: {sorted(list(found_values))}\")\n",
    "    #log(f\"Master values: {sorted(list(master_values))}\")\n",
    "    if found_values:\n",
    "        log(f\"Found values are of type {type(list(found_values)[0])}\")\n",
    "    else:\n",
    "        log(\"No 'found' vaules.\")\n",
    "    if master_values:\n",
    "        log(f\"Master values are of type {type(list(master_values)[0])}\")\n",
    "    else:\n",
    "        log(\"No 'master' values.\")\n",
    "    if culled_eids:\n",
    "        log(f\"Culled ids are of type {type(list(culled_eids)[0])}\")\n",
    "    else:\n",
    "        log(\"No culled eids.\")\n",
    "\n",
    "\n",
    "    missing_values = master_values - found_values - culled_eids\n",
    "    log(f\"{len(master_values)} user_ids found in master linkage CSV\")\n",
    "    log(f\"{len(found_values)} distinct ethica_id values found in telemetry CSV data\")\n",
    "    if missing_values: # filter D\n",
    "        log(f\"{len(missing_values)} expected {colname} values missing from {streamname} data\")\n",
    "        log(','.join([str(x) for x in sorted(list(missing_values))]))\n",
    "    else:\n",
    "        log(f\"All expected {colname} values found for {streamname}\")\n",
    "    unexpected_values = found_values - master_values\n",
    "    if unexpected_values: # filter E\n",
    "        log(f\"{len(unexpected_values)} {colname} values in {streamname} data were unexpected:\")\n",
    "        log(','.join([str(x) for x in sorted(list(unexpected_values))]))\n",
    "    else:\n",
    "        log(f\"All values found for {colname} in {streamname} were expected\")\n",
    "    #log(f\"Master values: {sorted(list(master_values))}\")\n",
    "    #log(f\"Found values: {sorted(list(found_values))}\")\n",
    "    #log(f\"Missing values: {sorted(list(missing_values))}\")\n",
    "    #log(f\"Unexpected values: {sorted(list(unexpected_values))}\")\n",
    "\n",
    "log(\"Code has been loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the block:** A collection of helper functions have been loaded, but nothing has been executed. The output above should simply confirm that the code has been loaded. If an error was reported, it is most likely either a syntax error in the code or a missing import statement. Whatever it was, that needs to be fixed before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:20:16.278685Z",
     "start_time": "2020-11-10T17:20:15.611243Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 7823, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 6762, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 6411, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 33869, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 35570, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 35990, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 35445, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 35671, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → Culling telemetry data from ethica user 35733, as instructed by linkage record\n",
      "[2022-12-01 09:53:14] → \n",
      "There were 644 participant records in linkage file '/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/../linkage.csv'\n",
      "[2022-12-01 09:53:14] → 558 were not Ethica users.\n",
      "[2022-12-01 09:53:14] → 86 were Ethica users.\n",
      "[2022-12-01 09:53:14] → 0 were flagged as 'ignore'. (Probably test users.)\n",
      "[2022-12-01 09:53:14] → 9 were flagged as 'cull'. (Probably due to known missing or corrupt data.)\n",
      "[2022-12-01 09:53:14] → There are now 86 known ethica ids loaded.\n",
      "[2022-12-01 09:53:14] → {1030, 7179, 7186, 7187, 33302, 33303, 33304, 7195, 7199, 7216, 7217, 35905, 33867, 32844, 33869, 33868, 7261, 7271, 6759, 6762, 7275, 6765, 33393, 35443, 35445, 35446, 35447, 35450, 35451, 35452, 35453, 35454, 35455, 35458, 6792, 7819, 4236, 7823, 35990, 36031, 36032, 36033, 9946, 35570, 35572, 6396, 34045, 34046, 6403, 6405, 6919, 6407, 6411, 6414, 7454, 35614, 5410, 6956, 5428, 9015, 33591, 9016, 33593, 6975, 6989, 35671, 35672, 5471, 34658, 34660, 34153, 34155, 6536, 35730, 35732, 35733, 35737, 35738, 8104, 8108, 35759, 34738, 8129, 35814, 7146, 7151}\n"
     ]
    }
   ],
   "source": [
    "# Now parse the linkage file and be sure it's well formed.\n",
    "\n",
    "ignorables = set([])\n",
    "usercount = 0\n",
    "notethica = 0\n",
    "\n",
    "# read the linkage file and validate key data fields\n",
    "with open(linkage_file,'r',encoding='ISO-8859-1') as fcsv:\n",
    "    reader = csv.DictReader(fcsv,delimiter=',')\n",
    "    # optimization, should probably reduce eid values to set\n",
    "    # of unique strings first and then validate those\n",
    "    for rownum,row in enumerate(list(reader)):\n",
    "        if row['ethica_id'] or row['interact_id'] or row['study_id']:\n",
    "            usercount += 1\n",
    "        #log(f\"Reading CSV row {rownum}\")\n",
    "        eid = row['ethica_id']\n",
    "        if not validate_eid(eid, logfailures=False): \n",
    "            #log(f\"CSV row #{rownum} has no eid\")\n",
    "            notethica += 1\n",
    "            continue # not an ethica row\n",
    "        if 'data_disposition' in row.keys():\n",
    "            if 'cull' in row['data_disposition']:\n",
    "                log(f\"Culling telemetry data from ethica user {eid}, as instructed by linkage record\")\n",
    "                culled_eids.add(int(eid))\n",
    "                # We leave the eid in the list, so that it will not show up as an unexpected id, \n",
    "                # but we also alert the operator that telemetry from this eid will be culled at ingest time\n",
    "            if 'ignore' in row['data_disposition']:\n",
    "                log(f\"Ignoring record for ethica user {eid}, as instructed by linkage record\")\n",
    "                ignorables.add(eid) # keep track of which IDs have been intentionally ignored\n",
    "                continue \n",
    "        #if not validate_eid(eid, logfailures=False):\n",
    "        #    notethica += 1\n",
    "         #   continue # we're only interested in ethica users\n",
    "        iid = row['interact_id']\n",
    "        if not iid or not validate_iid(iid, logfailures=False): # filter C \n",
    "            log(f\"Ethica user {eid} has no iid\")\n",
    "        iids.add(iid)\n",
    "log(f\"\\nThere were {usercount} participant records in linkage file '{linkage_file}'\")\n",
    "log(f\"{notethica} were not Ethica users.\")\n",
    "log(f\"{len(eids)} were Ethica users.\")\n",
    "log(f\"{len(ignorables)} were flagged as 'ignore'. (Probably test users.)\")\n",
    "log(f\"{len(culled_eids)} were flagged as 'cull'. (Probably due to known missing or corrupt data.)\")\n",
    "\n",
    "# add the ignorables to the eids so that they will not be\n",
    "# reported as unexpected\n",
    "eids = eids.union(ignorables)\n",
    "eids = set([int(x) for x in eids])\n",
    "\n",
    "log(f\"There are now {len(eids)} known ethica ids loaded.\")\n",
    "log(eids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the block:** The output above is telling you how many records were processed in the linkage.csv file, how many were using the Ethica app, and how many were not. If the number of Ethica users and non-Ethica users don't add up to the number of user records processed, that will be reported with a PROBLEM line instead of the usual CONFIRMED line. Figure out what's going on before continuing.\n",
    "\n",
    "Also look at the number of users reported as 'ignore' and 'cull'. These are users for whom we do not expect to find telemetry data. Initially, these will probably both be zero, but as we work our way through the process and discover test users, corrupt data, missing data, etc., we will flag those records in the linkage.csv so that the ingest protocol knows how to handle them. Be sure you understand both number reported here, and that they make sense to you, before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:20:30.932016Z",
     "start_time": "2020-11-10T17:20:16.280689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:53:21] → File /scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv is associated with study number 1318\n",
      "[2022-12-01 09:53:21] → WARNING: Unable to find file '/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv'\n",
      "[2022-12-01 09:53:21] → No 'found' vaules.\n",
      "[2022-12-01 09:53:21] → Master values are of type <class 'int'>\n",
      "[2022-12-01 09:53:21] → Culled ids are of type <class 'int'>\n",
      "[2022-12-01 09:53:21] → 86 user_ids found in master linkage CSV\n",
      "[2022-12-01 09:53:21] → 0 distinct ethica_id values found in telemetry CSV data\n",
      "[2022-12-01 09:53:21] → 77 expected ethica_id values missing from GPS data\n",
      "[2022-12-01 09:53:21] → 1030,4236,5410,5428,5471,6396,6403,6405,6407,6414,6536,6759,6765,6792,6919,6956,6975,6989,7146,7151,7179,7186,7187,7195,7199,7216,7217,7261,7271,7275,7454,7819,8104,8108,8129,9015,9016,9946,32844,33302,33303,33304,33393,33591,33593,33867,33868,34045,34046,34153,34155,34658,34660,34738,35443,35446,35447,35450,35451,35452,35453,35454,35455,35458,35572,35614,35672,35730,35732,35737,35738,35759,35814,35905,36031,36032,36033\n",
      "[2022-12-01 09:53:21] → All values found for ethica_id in GPS were expected\n",
      "[2022-12-01 09:53:21] → Participant Study Map has 0 users.\n",
      "[2022-12-01 09:53:21] → {}\n"
     ]
    }
   ],
   "source": [
    "# And now we can verify the GPS telemetry file against the individual records we just loaded.\n",
    "# But note that for cities with multiple telemetry directories, (ie Mtl), we need to examine\n",
    "# all the data folders together, and for each file, we need to keep track of which study it\n",
    "# is associated with.\n",
    "telemetrydirs = {os.path.join(telemetrydir, str(ethica_study_id), 'gps.csv'):ethica_study_id for ethica_study_id in study_ids[city_num]}\n",
    "detect_missing_values(eids, telemetrydirs, '2',  'ethica_id', 'GPS') #some coordinators use 'ethica_id', others use 'user_id'\n",
    "log(f\"Participant Study Map has {len(participant_study)} users.\")\n",
    "log(participant_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the block:** If everything is set up correctly, this block will load and report the list of user ids in the study. If it reports errors, this usually indicates that either the telemetrydir path has not been set properly, or that the data files in that directory have not yet been unpacked and given the expected names. Fix all reported errors and rerun the block. Do not proceed until this block has completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:41:18.324480Z",
     "start_time": "2020-11-10T17:20:30.934320Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:53:32] → WARNING: Processing XL data files takes approx. 10 times longer than GPS files. This step could take several hours to complete.\n",
      "[2022-12-01 09:53:32] → File /scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/accelerometer.csv is associated with study number 1318\n",
      "[2022-12-01 09:53:32] → WARNING: Unable to find file '/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/accelerometer.csv'\n",
      "[2022-12-01 09:53:32] → No 'found' vaules.\n",
      "[2022-12-01 09:53:32] → Master values are of type <class 'int'>\n",
      "[2022-12-01 09:53:32] → Culled ids are of type <class 'int'>\n",
      "[2022-12-01 09:53:32] → 86 user_ids found in master linkage CSV\n",
      "[2022-12-01 09:53:32] → 0 distinct ethica_id values found in telemetry CSV data\n",
      "[2022-12-01 09:53:32] → 77 expected user_id values missing from Accel data\n",
      "[2022-12-01 09:53:32] → 1030,4236,5410,5428,5471,6396,6403,6405,6407,6414,6536,6759,6765,6792,6919,6956,6975,6989,7146,7151,7179,7186,7187,7195,7199,7216,7217,7261,7271,7275,7454,7819,8104,8108,8129,9015,9016,9946,32844,33302,33303,33304,33393,33591,33593,33867,33868,34045,34046,34153,34155,34658,34660,34738,35443,35446,35447,35450,35451,35452,35453,35454,35455,35458,35572,35614,35672,35730,35732,35737,35738,35759,35814,35905,36031,36032,36033\n",
      "[2022-12-01 09:53:32] → All values found for user_id in Accel were expected\n",
      "[2022-12-01 09:53:32] → Participant Study Map has 0 users.\n"
     ]
    }
   ],
   "source": [
    "# And again for XL telemetry\n",
    "log(f\"WARNING: Processing XL data files takes approx. 10 times longer than GPS files. This step could take several hours to complete.\")\n",
    "telemetrydirs = {os.path.join(telemetrydir, str(ethica_study_id), 'accelerometer.csv'):ethica_study_id for ethica_study_id in study_ids[city_num]}\n",
    "detect_missing_values(eids, telemetrydirs, '2',  'user_id', 'Accel')\n",
    "log(f\"Participant Study Map has {len(participant_study)} users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the block:** Assuming the GPS verification passed, this XL verification block shouldn't find any problems with file names, so most errors here are likely to be related internal or logical problems within the data files. Fix them and get this block to run successfully before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Linkage\n",
    "\n",
    "Once the CSV file is known to be valid and complete, we can ingest those records into the ethica_assignments table, either adding records or updating them as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:41:31.865479Z",
     "start_time": "2020-11-10T17:41:22.686207Z"
    },
    "code_folding": [
     0,
     1
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '8129' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33302' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7216' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33304' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '9015' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6919' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6765' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6792' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7151' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '5428' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7823' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7271' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7179' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6405' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6407' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7199' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6956' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7217' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6762' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7275' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6396' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '9946' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '9016' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7261' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6536' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7187' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '4236' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6975' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7454' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '5410' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6989' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6414' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '1030' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7195' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7146' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7819' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '8108' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6411' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '8104' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6403' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '5471' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '6759' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '7186' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33303' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35672' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33867' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35732' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35443' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33869' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '32844' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34660' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35453' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34658' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35451' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35730' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35455' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35570' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34155' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35737' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35990' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35905' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35458' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33593' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35447' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35572' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34738' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35445' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35738' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35454' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '36031' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35671' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35733' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34045' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35446' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35814' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35759' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33393' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '36032' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33868' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '33591' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35614' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35452' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34046' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '34153' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID '36033' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID '35450' already ingested.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n",
      "[2022-12-01 09:53:42] → Ethica ID 'NA' is not an integer. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Create new users based on records found in linkage file\n",
    "create_sql = \"\"\"\n",
    "    INSERT INTO portal_dev.ethica_assignments (interact_id, ethica_id, study_id)\n",
    "    VALUES (%s, %s, %s);           \n",
    "    \"\"\"\n",
    "#log(f\"Participant study map: {participant_study}\")\n",
    "\n",
    "# Open a database session\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    # read through each line of the linkage csv\n",
    "    with open(linkage_file,'r') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        # for each line, determine whether the record already exists\n",
    "        for record in reader:\n",
    "            # figure out whether to update, insert, or delete, based on CSV record\n",
    "            ignore_user_rec = 'ignore' in record['data_disposition']\n",
    "            real_user = not 'cull' in record['data_disposition']\n",
    "            iid = record['interact_id']\n",
    "            eid = record['ethica_id']\n",
    "            if ignore_user_rec:\n",
    "                log(f\"Skipping creation of record for ethica_id {eid} based on 'ignore' flag.\")\n",
    "                continue\n",
    "            if not validate_eid(eid, logfailures=True): # ignore non-Ethica users\n",
    "                #log(f\"Skipping linkage record for IID {iid} - {eid} is not a valid Ethica ID\")\n",
    "                continue\n",
    "            #start_date = record['sd_start_1']\n",
    "            #end_date = record['sd_end_1']\n",
    "            if not real_user: # confirm user doesn't already exist\n",
    "                log(f\"Skipping creation of participation record for ethica test user {eid}\")\n",
    "                continue\n",
    "                \n",
    "            # determine which study the user should be assigned to         \n",
    "            if eid in participant_study:\n",
    "                assign_study = participant_study[eid]\n",
    "                log(f\"Assigning Ethica ID {eid} to study {assign_study}\")\n",
    "            # but if we don't already have that figured out, we can't just assign the id of the\n",
    "            # current study, because in some cases (like Mtl) there are multiple study ids and we\n",
    "            # don't know which one to assign.\n",
    "            else:\n",
    "                log(f\"Can't determine ethica_study_id for ethica_id {eid}. Skipping user.\")\n",
    "                continue\n",
    "            sql = f\"\"\"SELECT count(1) as count from portal_dev.ethica_assignments\n",
    "                      WHERE interact_id = {iid} AND study_id = {assign_study};\"\"\"\n",
    "            cur.execute(sql)\n",
    "            row = cur.fetchone()\n",
    "            if row['count'] == 0:   \n",
    "                cur.execute(create_sql, (iid, eid, assign_study))\n",
    "                if cur.rowcount == 1:\n",
    "                    log(f\"Interact User {iid} had no participation record for study {assign_study}. Created.\")\n",
    "                else:\n",
    "                    log(f\"Problem trying to create participation record for user {iid} in study {assign_study}\")\n",
    "                #log(f\"Creating user {iid}, {eid}, {assign_study}, {start_date}, {end_date}\")\n",
    "            else:\n",
    "                log(f\"INTERACT USER {iid} ALREADY EXISTS IN STUDY {assign_study}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "city_name": "Victoria",
     "city_prefix": "vic"
    }
   },
   "source": [
    "## Ingest Targets\n",
    "\n",
    "The preliminary ingest for {{city_name}} Wave 1 will create the following tables in the level_0 schema:\n",
    "\n",
    "  - {{city_prefix}}-w1-eth-gps-raw-TOXIC\n",
    "  - {{city_prefix}}-w1-eth-xls-raw-TOXIC\n",
    "  - {{city_prefix}}-w1-eth-xls-delduplrec (raw xls with all rec-time duplicates removed)\n",
    "  - {{city_prefix}}-w1-eth-xls-delconflrec (raw xls with all rec-time conflicts removed)\n",
    "  - {{city_prefix}}-w1-eth-gps-delduplsat (raw gps with all sat-time duplicates removed)\n",
    "  - {{city_prefix}}-w1-eth-gps-delduplrec (raw gps with all rec-time duplicates removed)\n",
    "  - {{city_prefix}}-w1-eth-gps-delconflsat (raw gps with all sat-time conflicts removed)\n",
    "  - {{city_prefix}}-w1-eth-gps-delconflrec (raw gps with all rec-time conflicts removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gps_raw_TOXIC\n",
    "\n",
    "Ideally, we want to use the psql COPY command, since it has been\n",
    "optimized for fast loading, but the target telemetry table and the\n",
    "incoming CSV file have different column names. We *COULD* just rename \n",
    "the columns in the CSV file, but that's a manual step that shouldn't\n",
    "be embedded into the process.\n",
    "\n",
    "So instead, we'll load the data into a temporary table, and then transfer it from there into the target table and match the records to their interact_ids at the same time. This method will more or less double the ingest time, but having a reliable ingest process that can be fully documented, without manual interventions, seems like the more robust path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:41:31.966249Z",
     "start_time": "2020-11-10T17:41:31.957860Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:54:14] → Code loaded.\n"
     ]
    }
   ],
   "source": [
    "# get a list of ethica_ids that are supposed to be culled from the ingest\n",
    "def get_ids_to_be_culled(csvfilepath):\n",
    "    culls = set([])\n",
    "    with open(csvfilepath,'r') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        # for each line, determine whether the record already exists\n",
    "        for record in reader:\n",
    "            #if 'cull' in record['data_disposition'] and not 'ignore' in record['data_disposition']:\n",
    "                # a record that is both culled and ignored would not have been counted in the expected row count\n",
    "            if 'cull' in record['data_disposition']:    \n",
    "                culls.add(record['ethica_id'])\n",
    "    if culls:\n",
    "        log(f\"Ethica IDs to be culled as per linkage file:\")\n",
    "        log(f\"{culls}\")\n",
    "    else:\n",
    "        log(\"Linkage file includes no cull requests.\")\n",
    "    return culls\n",
    "\n",
    "def get_ids_to_be_ignored(csvfilepath):\n",
    "    ignores = set([])\n",
    "    with open(csvfilepath,'r') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        # for each line, determine whether the record already exists\n",
    "        for record in reader:\n",
    "            #if 'cull' in record['data_disposition'] and not 'ignore' in record['data_disposition']:\n",
    "                # a record that is both culled and ignored would not have been counted in the expected row count\n",
    "            if 'ignore' in record['data_disposition']:    \n",
    "                ignores.add(record['ethica_id'])\n",
    "    if ignores:\n",
    "        log(f\"Ethica IDs to be ignored as per linkage file:\")\n",
    "        log(f\"{ignores}\")\n",
    "    else:\n",
    "        log(\"Linkage file includes no ignore requests.\")\n",
    "    return ignores\n",
    "\n",
    "log(f\"Code loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:41:32.631153Z",
     "start_time": "2020-11-10T17:41:31.970516Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:54:46] → Code loaded.\n"
     ]
    }
   ],
   "source": [
    "# Function to create the temporary incoming table and load the raw GPS data into it\n",
    "def load_raw_CSV_data(csvfilepaths, schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfersql):\n",
    "    log(f\"Loading files '{csvfilepaths}' into table '{tmptablename}' of schema '{schema}'\")\n",
    "    with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "        # Create the temporary onboarding table\n",
    "        fulltmptablename = f\"{schema}.{tmptablename}\"\n",
    "        sql = f\"\"\"\n",
    "            DROP TABLE IF EXISTS {fulltmptablename}; \n",
    "            CREATE TABLE {fulltmptablename} (\n",
    "                {tmpsqlvars}\n",
    "                );\n",
    "                \"\"\"\n",
    "        log(f\"Creating temp ingest table {fulltmptablename}\")\n",
    "        cur.execute(sql)\n",
    "\n",
    "        # now add a comment describing table's purpose\n",
    "        cur.execute(f\"COMMENT ON TABLE {fulltmptablename} IS 'Temporary table for raw data ingest.';\")\n",
    "        \n",
    "        # Now create the final destination table\n",
    "        # Create the final destination table\n",
    "        fulldesttablename = f\"{schema}.{desttablename}\"\n",
    "        sql = f\"\"\"\n",
    "            DROP TABLE IF EXISTS {fulldesttablename} CASCADE; \n",
    "            CREATE TABLE {fulldesttablename} (\n",
    "                {destsqlvars}\n",
    "                )\n",
    "                \"\"\"\n",
    "        log(f\"Creating destination table {fulldesttablename}\")\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        # With tables created, we can now load the data from the CSV files\n",
    "        for csvfilepath in csvfilepaths:\n",
    "            log(f\"Ingesting raw CSV from {csvfilepath} into {fulltmptablename}\")\n",
    "            with open(csvfilepath, 'r') as f:\n",
    "                # Notice that we don't need the `csv` module.\n",
    "                next(f) # Skip the header row because copy_from doesn't want it\n",
    "                cur.copy_expert(\n",
    "                    f\"COPY {fulltmptablename} FROM STDIN WITH (FORMAT CSV, HEADER FALSE);\",\n",
    "                    f,\n",
    "                    )\n",
    "                #cur.copy_from(f, fulltmptablename, sep=',')\n",
    "\n",
    "        # Now collect some basic stats and validate the loaded raw data\n",
    "        log(f\"Checking success of raw data load\")\n",
    "        rowcount = 0\n",
    "        for csvfilepath in csvfilepaths:\n",
    "            with open(csvfilepath,'r') as fh:\n",
    "                for line in fh:\n",
    "                    rowcount += 1\n",
    "            rowcount -= 1 #don't count the header row\n",
    "        log(f\"Expecting {rowcount:,} lines in table\")      \n",
    "        sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "                  FROM (\n",
    "                         SELECT COUNT(1) as num_recs\n",
    "                         FROM {fulltmptablename}\n",
    "                         GROUP BY user_id\n",
    "                       ) as records_per_user\n",
    "                  \"\"\"\n",
    "        cur.execute(sql)\n",
    "        row = cur.fetchone()\n",
    "        num_raw_recs = row['num_recs']\n",
    "        num_raw_users = row['num_users']\n",
    "        log(f\"Ingested {num_raw_recs:,} records across {num_raw_users:,} users.\")\n",
    "        if num_raw_recs == rowcount:\n",
    "            log(\"Raw data ingest appears successful.\")\n",
    "            conn.commit()\n",
    "        else:\n",
    "            log(f\"ERROR: INGESTED ROW COUNT ({num_raw_recs}) DOES NOT MATCH SOURCE FILE ({rowcount})\")\n",
    "            return\n",
    "              \n",
    "        # And finally, we can transfer the data from the staging table into the final location\n",
    "        log(f\"Matching ethica_ids to interact_ids and transfering records into destination table\")\n",
    "        cur.execute(transfer_sql)\n",
    "\n",
    "        # And collect some basic stats to validate the ingested data\n",
    "        log(f\"Checking success of ingest\")\n",
    "        sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "                  FROM (\n",
    "                         SELECT COUNT(1) as num_recs\n",
    "                         FROM {fulldesttablename}\n",
    "                         GROUP BY iid\n",
    "                       ) as records_per_user\n",
    "                  \"\"\"\n",
    "        cur.execute(sql)\n",
    "        row = cur.fetchone()\n",
    "        num_final_recs = row['num_recs']\n",
    "        num_final_users = row['num_users']\n",
    "        \n",
    "        # count the number of records for which there are no matching interact_ids\n",
    "        missing_sql = f\"\"\"\n",
    "            SELECT count(1) as num_missing_users, sum(num_recs) as total_missing_recs, min(interact_id)\n",
    "            FROM (\n",
    "                SELECT count(1) as num_recs, min(interact_id) as interact_id, min(user_id), min(ethica_email)\n",
    "                FROM \n",
    "                   portal_dev.ethica_assignments asgn\n",
    "                   RIGHT OUTER JOIN {fulltmptablename} raw\n",
    "                   ON asgn.ethica_id = raw.user_id\n",
    "                GROUP BY user_id\n",
    "            ) recs_per_unmatched_user\n",
    "            WHERE interact_id IS NULL;\n",
    "            \"\"\"\n",
    "        cur.execute(missing_sql)\n",
    "        row = cur.fetchone()\n",
    "        num_missing_recs = int(row['total_missing_recs'] or 0) #assign 0 if no records match\n",
    "        num_missing_users = int(row['num_missing_users'] or 0) #assign 0 if no records match\n",
    "        log(f\"Identified {num_missing_recs:,} records across {num_missing_users:,} users for whom interact_id is not known.\")\n",
    "        \n",
    "        # Report the success, partial success, or failure of the ingest\n",
    "        log(f\"Transfered {num_final_recs} records for {num_final_users} users.\")\n",
    "        return\n",
    "    \n",
    "        if num_raw_recs == num_final_recs:\n",
    "            log(\"Data transfer and ingest operation complete. All records ingested.\")\n",
    "            conn.commit()\n",
    "        elif num_raw_recs == num_final_recs + num_missing_recs:\n",
    "            log(f\"Data transfer and ingest operation complete. All records accounted for, but {num_missing_users:,} unknown users.\")\n",
    "            conn.commit()\n",
    "        else:\n",
    "            log(\"ERROR: TRANSFERED ROW COUNT DOES NOT MATCH STAGING TABLE\")\n",
    "            #log(f\"Expected {num_missing_recs} missing recs but found {} to be missing.\")\n",
    "   \n",
    "        \"\"\"    \n",
    "        A mismatched row count alone is not indicative of an ingest problem because some user data\n",
    "        is suppressed by design. Any user marked \"cull\" in the linkage table will be ignored, even\n",
    "        though they might have associated data in the telemetry file. (These are test users, for the \n",
    "        most part, but could also be problematic cases identified during ingest.)\n",
    "        \n",
    "        To create a more robust and accurate verification pass, we need to:\n",
    "        - have a list of all users who are being actively culled\n",
    "        - have a count of telemetry rows associated with each such culled account\n",
    "          - can count with join against user_category\n",
    "        - then compare the ingested raw ingest row count with the final ingest PLUS the culled rows\n",
    "        \"\"\"\n",
    "            \n",
    "        data = [ ['num_recs', num_final_recs], ['num_users', num_final_users], ]\n",
    "        print(f\"\\nFigure: Counts for {fulldesttablename}\")\n",
    "        print(tabulate(data, floatfmt=',.0f', stralign='right' ))\n",
    "        \n",
    "        \n",
    "        # If there were some unmatched user_ids, list them so operator can investigate\n",
    "        if num_missing_recs:\n",
    "            problems_sql = f\"\"\"\n",
    "                SELECT count(1) as num_recs, \n",
    "                       min(user_id) as user_id\n",
    "                FROM portal_dev.ethica_assignments asgn\n",
    "                   RIGHT OUTER JOIN {fulltmptablename} raw\n",
    "                   ON asgn.ethica_id = raw.user_id\n",
    "                WHERE interact_id IS NULL\n",
    "                GROUP BY user_id;\n",
    "                \"\"\"\n",
    "            print(f\"\\nFigure: Unmatched user_ids from {tmptablename}\")\n",
    "            cur.execute(problems_sql)\n",
    "            rows = cur.fetchall()\n",
    "            print(tabulate(rows, headers='keys'))\n",
    "            print(f\"Total Unmatched Rows: {num_missing_recs:,}\")\n",
    "#        else:\n",
    "#            log(\"Record count in CSV matches record count of ingested table\")\n",
    "log(f\"Code loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:42:40.707874Z",
     "start_time": "2020-11-10T17:41:32.640922Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 09:55:21] → Ethica IDs to be culled as per linkage file:\n",
      "[2022-12-01 09:55:21] → {'35990', 'NA', '6411', '7823', '35733', '35671', '6762', '33869', '35445', '35570'}\n",
      "[2022-12-01 09:55:21] → Linkage file includes no ignore requests.\n",
      "[2022-12-01 09:55:21] → Loading telemetry from files: {'/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv': 1318}\n",
      "[2022-12-01 09:55:21] → Loading files '{'/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv': 1318}' into table 'tmpgps' of schema 'level_0'\n",
      "[2022-12-01 09:55:21] → Creating temp ingest table level_0.tmpgps\n",
      "[2022-12-01 09:55:21] → Creating destination table level_0.ssk_w2_eth_gps_raw_TOXIC\n",
      "[2022-12-01 09:55:21] → Ingesting raw CSV from /scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv into level_0.tmpgps\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39749/1059377993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     }\n\u001b[1;32m     64\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading telemetry from files: {telemetryfiles}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mload_raw_CSV_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelemetryfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmptablename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpsqlvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesttablename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestsqlvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_sql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPS file ingest complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39749/127422690.py\u001b[0m in \u001b[0;36mload_raw_CSV_data\u001b[0;34m(csvfilepaths, schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfersql)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcsvfilepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsvfilepaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ingesting raw CSV from {csvfilepath} into {fulltmptablename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# Notice that we don't need the `csv` module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Skip the header row because copy_from doesn't want it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/lazarou/test_pipeline/permanent_archive/Saskatoon/Wave2/Ethica/raw/1318/gps.csv'"
     ]
    }
   ],
   "source": [
    "# Set up the parameters for loading the GPS table data from each study folder and then load it\n",
    "ids_to_cull = get_ids_to_be_culled(linkage_file)\n",
    "ids_to_ignore = get_ids_to_be_ignored(linkage_file)\n",
    "\n",
    "# make sure we cull any data from both cull records and ignore records\n",
    "ids_to_cull.update(ids_to_ignore)\n",
    "\n",
    "telemetryfilename = \"gps.csv\"\n",
    "#fullcsvpath = os.path.join(telemetrydir,  str(ethica_study_id), telemetryfilename)\n",
    "tmptablename = \"tmpgps\"\n",
    "desttablename = f\"{city_prefix}_w{wave_num}_eth_gps_raw_TOXIC\"\n",
    "\n",
    "tmpsqlvars = \"\"\"\n",
    "    study_id INTEGER NOT NULL,\n",
    "    user_id BIGINT NOT NULL,\n",
    "    date TEXT,\n",
    "    device_id TEXT NOT NULL,\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "    timestamp TEXT,\n",
    "    accu DOUBLE PRECISION,\n",
    "    alt DOUBLE PRECISION,\n",
    "    bearing DOUBLE PRECISION,\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    lon DOUBLE PRECISION NOT NULL,\n",
    "    provider TEXT,\n",
    "    satellite_time TIMESTAMP WITH TIME ZONE,\n",
    "    speed DOUBLE PRECISION\n",
    "    \"\"\"\n",
    "\n",
    "destsqlvars = \"\"\"\n",
    "    iid BIGINT NOT NULL,  -- interact_id\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL, -- participant's UTC time, to millisec, from phone clock\n",
    "    satellite_time TIMESTAMP WITH TIME ZONE NOT NULL, -- timestamp taken from satellite data\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    lon DOUBLE PRECISION NOT NULL,\n",
    "    speed DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    course DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    alt DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    accu DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    provider TEXT DEFAULT ''\n",
    "    \"\"\"\n",
    "\n",
    "transfer_sql = f\"\"\"\n",
    "   INSERT INTO {db_schema}.{desttablename} (iid,record_time,satellite_time,lat,lon,speed,course,alt,accu,provider)\n",
    "        SELECT asgn.interact_id,\n",
    "                raw.record_time,\n",
    "                raw.satellite_time,\n",
    "                raw.lat,\n",
    "                raw.lon,\n",
    "                raw.speed,\n",
    "                raw.bearing,\n",
    "                raw.alt,\n",
    "                raw.accu,\n",
    "                raw.provider\n",
    "        FROM {db_schema}.{tmptablename} raw \n",
    "            INNER JOIN portal_dev.ethica_assignments AS asgn\n",
    "            ON raw.user_id = asgn.ethica_id AND asgn.study_id = raw.study_id\n",
    "        WHERE raw.user_id not in {tuple(ids_to_cull)}\n",
    "            \"\"\"\n",
    "telemetryfiles = {\n",
    "    os.path.join(telemetrydir, str(ethica_study_id), 'gps.csv'): ethica_study_id\n",
    "    for ethica_study_id in study_ids[city_num]\n",
    "    }\n",
    "log(f\"Loading telemetry from files: {telemetryfiles}\")\n",
    "load_raw_CSV_data(telemetryfiles, db_schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfer_sql)\n",
    "    \n",
    "log(f\"GPS file ingest complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the block:** In addition to confirming that the ignored and culled users look right, also check that the number of users and records processed seem reasonable. Most studies should have on the order of 50-150 users and somewhere around 40-50,000 records per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T17:42:46.933517Z",
     "start_time": "2020-11-10T17:42:40.719818Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Validate the ingested table\n",
    "# Count the number of ingested records and compare with expected number\n",
    "desttablename = f\"{city_prefix}_w{wave_num}_eth_gps_raw_TOXIC\"\n",
    "fulldesttablename = f\"{db_schema}.{desttablename}\"\n",
    "\n",
    "# count records produced by users who were not ingested\n",
    "# First, generate the list of culled users\n",
    "culled_eids = get_ids_to_be_culled(linkage_file)\n",
    "ignored_eids = get_ids_to_be_ignored(linkage_file)\n",
    "log(f\"There were {len(culled_eids)} ethica_ids culled from the ingest.\")\n",
    "#log(f\"Culled IDs: {culled_eids}\")\n",
    "log(f\"There were {len(ignored_eids)} ethica_ids ignored from the ingest.\")\n",
    "#log(f\"Ignored IDs: {ignored_eids}\")\n",
    "\n",
    "# for the purposes of which ids are expected to have absent data, we include the ignored ids as well as culled\n",
    "culled_eids.update(ignored_eids)\n",
    "log(f\"All user_ids expected to have data missing: {culled_eids}\")\n",
    "\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    \n",
    "    # Next, count how many records are associated with those culled ids\n",
    "    sql = f\"\"\"\n",
    "           SELECT COUNT(1) AS total_samples, \n",
    "                  SUM(1) FILTER (WHERE user_id IN %s) AS num_culled_samples\n",
    "           FROM {db_schema}.{tmptablename}\"\"\"\n",
    "    cur.execute(sql, (tuple(culled_eids),))\n",
    "    row = cur.fetchone()\n",
    "    num_culled_samples = row['num_culled_samples'] if row['num_culled_samples'] else 0\n",
    "    total_raw_samples = row['total_samples']\n",
    "    \n",
    "    # Count number of actual records ingested into final table\n",
    "    sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "              FROM (\n",
    "                     SELECT COUNT(1) as num_recs\n",
    "                     FROM {fulldesttablename}\n",
    "                     GROUP BY iid\n",
    "                   ) as records_per_user\n",
    "              \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['total_raw_samples', total_raw_samples], \n",
    "            ['culled_samples', num_culled_samples],   \n",
    "            ['expected_final_samples', total_raw_samples - num_culled_samples],\n",
    "            ['actual_final_samples', row['num_recs']], ]\n",
    "\n",
    "    \n",
    "    if total_raw_samples - num_culled_samples == row['num_recs']:\n",
    "        log(\"Ingest passes basic validation.\")\n",
    "    else:\n",
    "        log(\"ERROR: INGEST VALIDATION FAILED\")\n",
    "        \n",
    "    print(f\"\\nFigure: Validating table '{fulldesttablename}'\\n\")\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xls_raw_TOXIC\n",
    "Proceeds as per the GPS table, but with accelerometry source file and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:10:20.754942Z",
     "start_time": "2020-11-10T17:42:46.935566Z"
    },
    "code_folding": [
     0,
     12,
     24
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the parameters for loading the XL table data and then load it\n",
    "telemetryfilename = \"accelerometer.csv\"\n",
    "#fullcsvpath = os.path.join(telemetrydir, telemetryfilename)\n",
    "tmptablename = \"tmpxl\"\n",
    "desttablename = f\"{city_prefix}_w{wave_num}_eth_xls_raw_TOXIC\"\n",
    "\n",
    "ids_to_cull = get_ids_to_be_culled(linkage_file)\n",
    "ids_to_ignore = get_ids_to_be_ignored(linkage_file)\n",
    "\n",
    "# make sure we cull any data from both cull records and ignore records\n",
    "ids_to_cull.update(ids_to_ignore)\n",
    "\n",
    "tmpsqlvars = \"\"\"\n",
    "    study_id INTEGER NOT NULL,\n",
    "    user_id BIGINT NOT NULL,\n",
    "    date TEXT,\n",
    "    device_id TEXT NOT NULL,\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "    timestamp TEXT,\n",
    "    accu DOUBLE PRECISION,\n",
    "    x_axis DOUBLE PRECISION NOT NULL,\n",
    "    y_axis DOUBLE PRECISION NOT NULL,\n",
    "    z_axis DOUBLE PRECISION NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "destsqlvars = \"\"\"\n",
    "    iid BIGINT NOT NULL,   -- interact_id\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL, -- participant's UTC time, to millisec \n",
    "    x DOUBLE PRECISION NOT NULL,\n",
    "    y DOUBLE PRECISION NOT NULL,\n",
    "    z DOUBLE PRECISION NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "transfer_sql = f\"\"\"\n",
    "    INSERT INTO {db_schema}.{desttablename} (iid,record_time,x,y,z)\n",
    "    SELECT asgn.interact_id,\n",
    "        raw.record_time,\n",
    "        raw.x_axis,\n",
    "        raw.y_axis,\n",
    "        raw.z_axis\n",
    "    FROM {db_schema}.{tmptablename} raw \n",
    "        INNER JOIN portal_dev.ethica_assignments asgn\n",
    "        ON raw.user_id = asgn.ethica_id AND asgn.study_id = raw.study_id\n",
    "        WHERE raw.user_id not in {tuple(ids_to_cull)}\n",
    "        \"\"\"\n",
    "\n",
    "#telemetryfiles = {os.path.join(telemetrydir, 'accelerometer.csv'):ethica_study_id,}\n",
    "#load_raw_CSV_data(telemetryfiles, db_schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfer_sql)\n",
    "\n",
    "telemetryfiles = {os.path.join(telemetrydir, str(ethica_study_id), 'accelerometer.csv'):ethica_study_id for ethica_study_id in study_ids[city_num]}\n",
    "log(f\"Loading telemetry from files: {telemetryfiles}\")\n",
    "load_raw_CSV_data(telemetryfiles, db_schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfer_sql)\n",
    "    \n",
    "log(f\"XL file ingest complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:53.624469Z",
     "start_time": "2020-11-10T19:10:25.183097Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate the ingested table\n",
    "# Count the number of ingested records and compare with expected number\n",
    "desttablename = f\"{city_prefix}_w{wave_num}_eth_xls_raw_TOXIC\"\n",
    "fulldesttablename = f\"{db_schema}.{desttablename}\"\n",
    "tmptablename = \"tmpxl\"\n",
    "\n",
    "log(f\"Validating {desttablename}...\")\n",
    "# count records produced by users who were not ingested\n",
    "# First, generate the list of users whose telemetry is being culled\n",
    "culled_eids = get_ids_to_be_culled(linkage_file)\n",
    "ignored_eids = get_ids_to_be_ignored(linkage_file)\n",
    "log(f\"There were {len(culled_eids)} ethica_ids culled from the ingest.\")\n",
    "#log(f\"Culled IDs: {culled_eids}\")\n",
    "log(f\"There were {len(ignored_eids)} ethica_ids ignored from the ingest.\")\n",
    "#log(f\"Ignored IDs: {ignored_eids}\")\n",
    "\n",
    "# for the purposes of which ids are expected to have absent data, we include the ignored ids as well as culled\n",
    "culled_eids.update(ignored_eids)\n",
    "\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "    # Next, count how many records are associated with those culled ids\n",
    "    sql = f\"\"\"\n",
    "           SELECT COUNT(1) AS total_samples, \n",
    "                  SUM(1) FILTER (WHERE user_id IN %s) AS num_culled_samples\n",
    "           FROM {db_schema}.{tmptablename}\"\"\"\n",
    "    cur.execute(sql, (tuple(culled_eids),))\n",
    "    row = cur.fetchone()\n",
    "    num_culled_samples = row['num_culled_samples'] if row['num_culled_samples'] else 0\n",
    "    total_raw_samples = row['total_samples']\n",
    "    \n",
    "    # Count number of actual records ingested into final table\n",
    "    sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "              FROM (\n",
    "                     SELECT COUNT(1) as num_recs\n",
    "                     FROM {fulldesttablename}\n",
    "                     GROUP BY iid\n",
    "                   ) as records_per_user\n",
    "              \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['total_raw_samples', total_raw_samples], \n",
    "            ['culled_samples', num_culled_samples],   \n",
    "            ['expected_final_samples', total_raw_samples - num_culled_samples],\n",
    "            ['actual_final_samples', row['num_recs']], ]\n",
    "\n",
    "    \n",
    "    if total_raw_samples - num_culled_samples == row['num_recs']:\n",
    "        log(\"Ingest passes basic validation.\")\n",
    "    else:\n",
    "        log(\"ERROR: INGEST VALIDATION FAILED\")\n",
    "        \n",
    "    print(f\"\\nFigure: Validating table '{fulldesttablename}'\\n\")\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing population of both tables\n",
    "As a final validation, verify that the GPS and XLS raw tables both have data from the same users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:18:23.713213Z",
     "start_time": "2020-11-10T19:14:53.629950Z"
    }
   },
   "outputs": [],
   "source": [
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "   \n",
    "    mismatch_sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS tmpgpsiids;\n",
    "        DROP TABLE IF EXISTS tmpxlsiids;\n",
    "        CREATE TABLE tmpgpsiids AS SELECT DISTINCT iid FROM {city_prefix}_w{wave_num}_eth_gps_raw_toxic ;\n",
    "        CREATE TABLE tmpxlsiids AS SELECT DISTINCT iid FROM {city_prefix}_w{wave_num}_eth_xls_raw_toxic ;\n",
    "        SELECT tmpgpsiids.iid AS gps_iid, tmpxlsiids.iid AS xls_iid \n",
    "        FROM tmpgpsiids \n",
    "             RIGHT OUTER JOIN tmpxlsiids \n",
    "             ON tmpgpsiids.iid = tmpxlsiids.iid \n",
    "        WHERE tmpgpsiids.iid IS NULL \n",
    "           OR tmpxlsiids.iid IS NULL;\n",
    "        \"\"\"\n",
    "    \n",
    "    print(f\"\\nFigure: {city_name} users with data present in only one table\\n\")\n",
    "    cur.execute(mismatch_sql)\n",
    "    rows = cur.fetchall()\n",
    "    if rows:\n",
    "        print(tabulate(rows, headers='keys', stralign='right'))\n",
    "    else:\n",
    "        print('No mismatches found.')\n",
    "\n",
    "    # Interact_id 302562672 is known to have no GPS data for SSK\n",
    "    # Interact_id 201680208 is known to have no GPS data for VAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table *-xs-delduplrec \n",
    "This table will be extracted from the raw XLS data, with all record_time **duplicates** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:00:59.550201Z",
     "start_time": "2020-11-10T19:18:24.267356Z"
    }
   },
   "outputs": [],
   "source": [
    "tablename = f\"{city_prefix}_w{wave_num}_eth_xls_delduplrec\"\n",
    "sourcetable = f\"{city_prefix}_w{wave_num}_eth_xls_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} from scratch...\")\n",
    "    sql = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {db_schema}.{tablename};\n",
    "        CREATE TABLE {db_schema}.{tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, x, y, z\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time,\n",
    "                       min(x) as x,\n",
    "                       min(y) as y,\n",
    "                       min(z) as z\n",
    "                FROM {db_schema}.{sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1; -- there are no duplicates at all\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {db_schema}.{tablename} (iid, record_time);\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created table\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {db_schema}.{tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table *-gps-delduplrec \n",
    "This table will be extracted from the raw GPS data, with all record_time **duplicates** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:01:20.232735Z",
     "start_time": "2020-11-10T20:01:04.346216Z"
    }
   },
   "outputs": [],
   "source": [
    "tablename = f'{city_prefix}_w{wave_num}_eth_gps_delduplrec'\n",
    "sourcetable = f\"{city_prefix}_w{wave_num}_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating table {tablename} from scratch...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS {tablename};\n",
    "        CREATE TABLE {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time, \n",
    "                       min(satellite_time) as satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1; -- there are no duplicates at all\n",
    "            CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created table\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table *-gps-delconflrec \n",
    "This table will be extracted from the raw GPS data, with all record_time **conflicts** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:01:35.976004Z",
     "start_time": "2020-11-10T20:01:20.236192Z"
    }
   },
   "outputs": [],
   "source": [
    "tablename = f'{city_prefix}_w{wave_num}_eth_gps_delconflrec'\n",
    "sourcetable = f\"{city_prefix}_w{wave_num}_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating table {tablename} from scratch...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS {tablename};\n",
    "        CREATE TABLE {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time, \n",
    "                       min(satellite_time) as satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "               OR (minlon = maxlon AND minlat = maxlat); -- the duplicates are identical\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created table\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table *-xls-delconflrec \n",
    "This table will be extracted from the raw XL data, with all record_time **conflicts** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:45:26.362245Z",
     "start_time": "2020-11-10T20:01:35.979186Z"
    }
   },
   "outputs": [],
   "source": [
    "tablename = f'{city_prefix}_w{wave_num}_eth_xls_delconflrec'\n",
    "sourcetable = f\"{city_prefix}_w{wave_num}_eth_xls_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating table {tablename} from scratch...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS {tablename};\n",
    "        CREATE TABLE {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, minx as x, miny as y, minz as z\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time,\n",
    "                       min(x) as minx,\n",
    "                       max(x) as maxx,\n",
    "                       min(y) as miny,\n",
    "                       max(y) as maxy,\n",
    "                       min(z) as minz,\n",
    "                       max(z) as maxz\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "               OR (minx = maxx AND miny = maxy AND minz = maxz); -- the duplicates are identical\n",
    "            CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created table\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table *-gps-delduplsat\n",
    "This table will be extracted from the raw GPS data, with all satellite_time **duplicates** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:45:44.455195Z",
     "start_time": "2020-11-10T20:45:30.945976Z"
    }
   },
   "outputs": [],
   "source": [
    "tablename = f'{city_prefix}_w{wave_num}_eth_gps_delduplsat'\n",
    "sourcetable = f\"{city_prefix}_w{wave_num}_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating table {tablename} from scratch...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS {tablename};\n",
    "        CREATE TABLE {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       min(record_time) as record_time, \n",
    "                       satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, satellite_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1; -- there are no duplicates at all\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, satellite_time);\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created table\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table *-gps-delconflsat\n",
    "This table will be extracted from the raw GPS data, with all satellite_time **conflicts** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:45:59.148622Z",
     "start_time": "2020-11-10T20:45:44.462619Z"
    }
   },
   "outputs": [],
   "source": [
    "tablename = f'{city_prefix}_w{wave_num}_eth_gps_delconflsat'\n",
    "sourcetable = f\"{city_prefix}_w{wave_num}_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating table {tablename} from scratch...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS {tablename};\n",
    "        CREATE TABLE {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       min(record_time) as record_time, \n",
    "                       satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, satellite_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "               OR (minlon = maxlon AND minlat = maxlat); -- the duplicates are identical\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, satellite_time);\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created table\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T20:47:06.423329Z",
     "start_time": "2020-11-10T20:45:59.153156Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "tablename = f'{city_prefix}_w{wave_num}_eth_xls_delduplrec'\n",
    "# Quick block that can be run at any time to validate counts on any of the above-created tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database=db_name) as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    \n",
    "    # Collect the same basic stats as computed during ingest\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(f\"Figure: Basic counts for table {tablename}\")\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here endeth the ingest."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
